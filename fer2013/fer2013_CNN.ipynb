{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fer2013_CNN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6zj1jLdF3dkl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":126},"outputId":"f96773b3-7898-4fcf-d31f-fede4d263bdf","executionInfo":{"status":"ok","timestamp":1591323417867,"user_tz":480,"elapsed":21464,"user":{"displayName":"Daniel Tseng","photoUrl":"","userId":"05118626222774837985"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u2QX02_S3wCD","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/content/drive/Shared drives/Emotion Classification Through Deep Learning Drive/fer2013')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1jBtijrJ3jb","colab_type":"code","outputId":"08786c1e-1a39-4286-909b-db5924241ded","executionInfo":{"status":"ok","timestamp":1590625040231,"user_tz":420,"elapsed":2683,"user":{"displayName":"George Gabricht","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgF6OVYE4hVhnj7TIauxvLzLfbkf_0S-qHKAkoA=s64","userId":"04106723924461638279"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","# based off of https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/\n","# %%\n","from numpy import mean\n","from numpy import std\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import KFold\n","from keras.datasets import mnist\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import AveragePooling2D\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import Flatten\n","from keras.layers import BatchNormalization\n","from keras.optimizers import SGD\n","import keras\n","from fer2013_loader import fer2013\n","from keras.utils.vis_utils import plot_model\n","from keras.models import load_model\n","\n","import time\n","\n","def mnist_data():\n","# load dataset\n","    (trainX, trainY), (testX, testY) = mnist.load_data()\n","    # reshape dataset to have a single channel\n","\n","    #making sure data is correct dimensions\n","    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n","    testX = testX.reshape((testX.shape[0], 28, 28, 1))\n","\n","    #making sure targets are categorical\n","    trainY = to_categorical(trainY)\n","    testY = to_categorical(testY)\n","\n","    return trainX,trainY, testX,testY\n","\n","def prep_pixels(train, test):\n","\t# convert from integers to floats\n","\ttrain_norm = train.astype('float32')\n","\ttest_norm = test.astype('float32')\n","\t# normalize to range 0-1\n","\ttrain_norm = train_norm / 255.0\n","\ttest_norm = test_norm / 255.0\n","\t# return normalized images\n","\treturn train_norm, test_norm\n","# define cnn model"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"us0bI3CSJ5_G","colab_type":"code","colab":{}},"source":["# ~ 79% Accuracy\n","def define_model(input_shape=(28,28,1),output_shape=10):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(MaxPooling2D((2, 2)))\n","    \n","    model.add(Flatten())\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(output_shape, activation='softmax'))\n","    # compile model\n","    opt = SGD(lr=0.005, momentum=0.9)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HiSmBWde3iRi","colab_type":"code","colab":{}},"source":["\n","# based off of https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/\n","# %%\n","\n","# %%\n","####Uncomment to compare mnist and fer2013 dataset\n","#trainX,trainY, testX,testY = mnist_data()\n","#data = fer2013()\n","#data.shuffle(seed=10)\n","#x_train,x_test,y_train,y_test = data.split_data()\n","#\n","#print(x_train.shape,trainX.shape)\n","#print(y_train.shape,trainY.shape)\n","\n","\n","# %%\n","#uncommdt for mnist dataset\n","#trainX,trainY, testX,testY = mnist_data()\n","#train_norm,test_norm = prep_pixels(trainX,testX)\n","\n","# %%\n","#uncomment for fer2013 dataset\n","\n","data = fer2013()\n","data.augment()\n","data.shuffle(seed=10)\n","data.normalize()\n","trainX,trainY, testX,testY = data.split_data(seed=11)\n","train_norm, test_norm = trainX,testX\n","# %%\n","startTime = time.time()\n","print('Input image size: ',trainX[0].shape)\n","model = define_model(input_shape=trainX[0].shape,output_shape=trainY.shape[1])\n","plot_model(model, to_file='/content/drive/Shared drives/Emotion Classification Through Deep Learning Drive/fer2013/models/model_%f.png'%startTime, show_shapes=True, show_layer_names=True)\n","print('Data Size/num. parameters: %0.3f'%(len(train_norm)/model.count_params()))\n","# %%\n","history = model.fit(train_norm, trainY, epochs=50, batch_size=32, validation_split=0.2, verbose=1,shuffle=True)\n","model.save('/content/drive/Shared drives/Emotion Classification Through Deep Learning Drive/fer2013/models/model_colab_%f.h5'%startTime)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1AKpTiDyyFt","colab_type":"code","colab":{}},"source":["  plt.figure()\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('model accuracy')\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'val'], loc='upper left')\n","  plt.show()\n","  # %%\n","\n","  shuffledY = testY*1.0\n","  randomState = np.random.RandomState(seed=12)\n","  randomState.shuffle(shuffledY)\n","\n","  results = model.evaluate(test_norm, testY, batch_size=128)\n","  print('test loss, test acc:', results)\n","  pred = model.predict(test_norm)\n","  squareSize = 5\n","  counter = 0\n","\n","  for kk in range(5):\n","      fig, ax = plt.subplots(ncols=squareSize,nrows=squareSize,figsize = (15,10))\n","\n","      for ii in range(squareSize):\n","          for jj in range(squareSize):\n","              if np.argmax(testY[counter])==np.argmax(pred[counter]):\n","                  color='blue'\n","              else:\n","                  color='darkred'\n","\n","              ax[ii,jj].imshow(testX[counter][:,:,0],cmap='gray')\n","              ax[ii,jj].set_title('T: %s\\nP: %s  P. Val: %0.2f'%(data.label_key(np.argmax(testY[counter])),data.label_key(np.argmax(pred[counter])),np.max(pred[counter])),color=color)\n","              ax[ii,jj].axis('off')\n","              counter +=1\n","\n","      plt.tight_layout()\n","      plt.savefig('/content/drive/Shared drives/Emotion Classification Through Deep Learning Drive/fer2013/%05d.png'%kk)\n","      plt.close()\n","  # %%\n","  print('FER2013 Class Breakdown')\n","  binnedY = np.sum(testY,axis=0)\n","  binnedPred = np.sum(pred,axis=0)\n","  setSize = len(testY)\n","  for ii in range(len(testY[0])):\n","      print(data.label_key(ii) + ' %0.2f '%(binnedY[ii]/setSize))\n","  print('Errors')\n","  for ii in range(len(testY[0])):\n","      print(data.label_key(ii) + ' %0.3f '%(np.abs((binnedY[ii]-binnedPred[ii]))/binnedY[ii]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qoe-ARn9T35Q","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","# ***George's Experiments***\n","\n","\n","---\n","\n","\n","### **Instructions:**\n","1.   Run Cells 1 - 3\n","2.   Pick a Model in a define_model function and run that cell\n","3.   Run cells for generate_folds, augment_images, cross_validate_model and analyze_results functions.\n","4.   Run the final cell which calls these functions, with desired params.\n","5.   To Repeat for another model, repeat steps 2 - 4.\n","\n","---\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Lm5vcJBnJ6HA","colab_type":"code","colab":{}},"source":["# ~ Average Test Loss: 0.4558829044304239 | Average Test Accuracy: 0.9494541049003601\n","def define_model(input_shape=(28,28,1),output_shape=10):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(MaxPooling2D((2, 2)))\n","\n","    model.add(Flatten())\n","    model.add(Dense(400, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(output_shape, activation='softmax'))\n","    # compile model\n","    opt = SGD(lr=0.005, momentum=0.9)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zve5gIaPOlbm","colab_type":"code","colab":{}},"source":["# ~ Average Test Loss: 0.6459529956183779 | Average Test Accuracy: 0.9230504393577575\n","def define_model(input_shape=(28,28,1),output_shape=10):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(AveragePooling2D((2, 2)))\n","\n","    model.add(Flatten())\n","    model.add(Dense(400, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(output_shape, activation='softmax'))\n","    # compile model\n","    opt = SGD(lr=0.005, momentum=0.9)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BbUchq9RfEyb","colab_type":"code","colab":{}},"source":["# ~ Average Test Loss: 0.6580276918334576 | Average Test Accuracy: 0.9190239787101746\n","def define_model(input_shape=(28,28,1),output_shape=10):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(AveragePooling2D((2, 2)))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","\n","    model.add(Flatten())\n","    model.add(Dense(400, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(output_shape, activation='softmax'))\n","    # compile model\n","    opt = SGD(lr=0.005, momentum=0.9)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1OYHBbffSD0","colab_type":"code","colab":{}},"source":["# ~ Average Test Loss: 0.7062697377970435 | Average Test Accuracy: 0.9221169590950012\n","def define_model(input_shape=(28,28,1),output_shape=10):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(AveragePooling2D((2, 2)))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(MaxPooling2D((2, 2)))\n","\n","    model.add(Flatten())\n","    model.add(Dense(400, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(output_shape, activation='softmax'))\n","    # compile model\n","    opt = SGD(lr=0.005, momentum=0.9)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"78ttNIOMfaNd","colab_type":"code","colab":{}},"source":["# ~ % Accuracy\n","def define_model(input_shape=(28,28,1),output_shape=10):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(AveragePooling2D((2, 2)))\n","\n","    model.add(Flatten())\n","    model.add(Dense(400, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(output_shape, activation='softmax'))\n","    # compile model\n","    opt = SGD(lr=0.005, momentum=0.9)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"in8E9gp1n1t4","colab_type":"code","colab":{}},"source":["# ~ % Accuracy\n","def define_model(input_shape=(28,28,1),output_shape=10):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    \n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(AveragePooling2D((2, 2)))\n","\n","    model.add(Flatten())\n","    model.add(Dense(400, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(output_shape, activation='softmax'))\n","    # compile model\n","    opt = SGD(lr=0.005, momentum=0.9)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6Ccon92o1fp","colab_type":"code","colab":{}},"source":["# ~ % Accuracy\n","def define_model(input_shape=(28,28,1),output_shape=10):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(AveragePooling2D((2, 2)))\n","\n","    model.add(Flatten())\n","    model.add(Dense(400, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(output_shape, activation='softmax'))\n","    # compile model\n","    opt = SGD(lr=0.005, momentum=0.9)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k2PI8G6sv9Cw","colab_type":"code","colab":{}},"source":["# ~ Test Time: 05/10/2020 09:07:09 | Average Test Loss: 0.4399232642226038 | Average Test Accuracy: 0.952156913280487\n","def define_model(input_shape=(28,28,1),output_shape=10):\n","    model = Sequential()\n","\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(AveragePooling2D((2, 2)))\n","\n","    model.add(Flatten())\n","\n","    model.add(Dense(1200, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(400, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n","\n","    model.add(Dense(output_shape, activation='softmax'))\n","    # compile model\n","    opt = SGD(lr=0.005, momentum=0.9)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PxUdKzK6uG9H","colab_type":"code","colab":{}},"source":["# ~ Test Time: 05/13/2020 23:01:30 | Average Test Loss: 0.2842309197449503 | Average Test Accuracy: 0.9473639726638794\n","def define_model(input_shape=(28,28,1),output_shape=10):\n","    model = Sequential()\n","\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Dropout(0.5))\n","\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(AveragePooling2D((2, 2)))\n","    model.add(Dropout(0.5))\n","\n","    model.add(Flatten())\n","\n","    model.add(Dense(1200, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(400, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n","\n","    model.add(Dense(output_shape, activation='softmax'))\n","    # compile model\n","    opt = SGD(lr=0.005, momentum=0.9)\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpfxNjUaxeqO","colab_type":"code","colab":{}},"source":["def generate_folds(n_splits, X_data, y_data):\n","  for train_index, test_index in KFold(n_splits).split(X_data):\n","    X_train, X_test = X_data[train_index], X_data[test_index]\n","    y_train, y_test = y_data[train_index], y_data[test_index]\n","    yield X_train,y_train,X_test,y_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UoN3itwDM8z9","colab_type":"code","colab":{}},"source":["def augment_images(train_norm, trainY, copyRatio=1.0, copyMult=5):\n","  temp_train_norm = train_norm\n","  temp_trainY = trainY\n","  \n","  datagen = ImageDataGenerator(\n","        rotation_range=40,\n","        width_shift_range=0.2,\n","        height_shift_range=0.2,\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        fill_mode='nearest')\n","  \n","  saved_images = []\n","  print(\"Generating Images in: range(0,\",len(train_norm),\",\",((int) (1 / copyRatio)),\")\\n\")\n","  for ndxSrc in range(0,len(train_norm), ((int) (1 / copyRatio))):\n","  #for ndxSrc in range(len(train_norm)):\n","    ndxGen = 0\n","    for newImg, newTarget in datagen.flow(train_norm[ndxSrc::], trainY[ndxSrc::], batch_size=1):\n","      np.append(temp_train_norm, newImg)\n","      np.append(temp_trainY, newTarget)\n","      if (ndxGen >= copyMult):\n","        saved_images.append((newImg, newTarget))\n","        break\n","      ndxGen += 1  \n","\n","  for ndx in range(min(len(saved_images), 100)):\n","    plt.imshow(array_to_img(saved_images[ndx][0][0]))\n","    plt.show()\n","    print(f\"Target: {saved_images[ndx][1]}\")\n","\n","  return (temp_train_norm, temp_trainY) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vldu_KLWhPDl","colab_type":"code","colab":{}},"source":["def mirror_images(pixels, targets):\n","  new_pixels = pixels*1.0\n","  for ndx in range(len(new_pixels)):\n","      new_pixels[ndx] = np.fliplr(new_pixels[ndx][:,:,0]).reshape((len(new_pixels[0]),len(new_pixels[0][0]),1))\n","  pixels = np.vstack((pixels,new_pixels))\n","  targets = np.vstack((targets,targets))\n","\n","  return (pixels, targets)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LF6x38PWmnQ2","colab_type":"code","colab":{}},"source":["from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n","def cross_validate_model(num_epochs=30, seed=10, num_folds=5, batch_size=32, validation_split=0.2, augment=False):\n","  data = fer2013()\n","  #data.pixels = np.reshape(data.pixels, (data.pixels.shape[0], 1, data.pixels.shape[1], data.pixels.shape[2]))\n","  #print(data.pixels.shape)\n","  #data.augment()\n","  data.shuffle(seed=int(0.8 * seed))\n","  data.shuffle(seed=seed)\n","  data.shuffle(seed=int(1.4 * seed))\n","  #print(data.pixels.shape)\n","  data.normalize()\n","\n","  #print(data.pixels.shape)\n","  #data.pixels = np.reshape(data.pixels, (data.pixels.shape[0], 1, data.pixels.shape[1], data.pixels.shape[2]))\n","\n","  histories = []\n","  models = []\n","  tests = []\n","  for trainX,trainY,testX,testY in generate_folds(num_folds, data.pixels, data.targets):\n","    # %%\n","    startTime = time.time()\n","\n","    train_norm, test_norm = trainX,testX\n","    \n","    if (augment):\n","      train_norm, trainY = augment_images(train_norm, trainY, copyRatio=0.005, copyMult=1)\n","    else:\n","      train_norm, trainY = mirror_images(train_norm, trainY)\n","\n","    model = define_model(input_shape=train_norm[0].shape,output_shape=trainY.shape[1])\n","    plot_model(model, to_file='/content/drive/Shared drives/Emotion Classification Through Deep Learning Drive/fer2013/models/model_%f.png'%startTime, show_shapes=True, show_layer_names=True)\n","    print('Data Size/num. parameters: %0.3f'%(len(train_norm)/model.count_params()))\n","    # %%\n","    #history = model.fit(datagen.flow(train_norm, trainY, batch_size=BATCH_SIZE, shuffle=True), steps_per_epoch=len(train_norm) / 32, epochs = NUM_EPOCHS)\n","    history = model.fit(train_norm, trainY, epochs=num_epochs, batch_size=batch_size, validation_split=validation_split, verbose=1,shuffle=False)\n","    models.append(model)\n","    histories.append(history)\n","    tests.append((test_norm, testY))\n","    model.save('/content/drive/Shared drives/Emotion Classification Through Deep Learning Drive/fer2013/models/model_colab_%f.h5'%startTime)\n","  return (data, histories, models, tests)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KrDCpXLg5_Si","colab_type":"code","colab":{}},"source":["from datetime import datetime\n","\n","def analyze_results(data=None, histories=None, models=None, tests=None):\n","  sum_results = (0.0, 0.0)\n","  size_results = 0\n","\n","  for ndx in range(len(histories)):\n","    history = histories[ndx]\n","    model = models[ndx]\n","    test_norm, testY = tests[ndx]\n","\n","    plt.figure()\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.show()\n","    # %%\n","\n","    shuffledY = testY*1.0\n","    randomState = np.random.RandomState(seed=12)\n","    randomState.shuffle(shuffledY)\n","\n","    results = model.evaluate(test_norm, testY, batch_size=128)\n","    print('test loss, test acc:', results)\n","    sum_results = (sum_results[0] + results[0], sum_results[1] + results[1])\n","    size_results += 1\n","    pred = model.predict(test_norm)\n","    squareSize = 5\n","    counter = 0\n","\n","    if (data != None):\n","      for kk in range(5):\n","          fig, ax = plt.subplots(ncols=squareSize,nrows=squareSize,figsize = (15,10))\n","\n","          for ii in range(squareSize):\n","              for jj in range(squareSize):\n","                  if np.argmax(testY[counter])==np.argmax(pred[counter]):\n","                      color='blue'\n","                  else:\n","                      color='darkred'\n","\n","                  ax[ii,jj].imshow(test_norm[counter][:,:,0],cmap='gray')\n","                  ax[ii,jj].set_title('T: %s\\nP: %s  P. Val: %0.2f'%(data.label_key(np.argmax(testY[counter])),data.label_key(np.argmax(pred[counter])),np.max(pred[counter])),color=color)\n","                  ax[ii,jj].axis('off')\n","                  counter +=1\n","\n","          plt.tight_layout()\n","          plt.savefig('/content/drive/Shared drives/Emotion Classification Through Deep Learning Drive/fer2013/%05d.png'%kk)\n","          plt.close()\n","\n","      print('FER2013 Class Breakdown')\n","      binnedY = np.sum(testY,axis=0)\n","      binnedPred = np.sum(pred,axis=0)\n","      setSize = len(testY)\n","      for ii in range(len(testY[0])):\n","          print(data.label_key(ii) + ' %0.2f '%(binnedY[ii]/setSize))\n","      print('Errors')\n","      for ii in range(len(testY[0])):\n","          print(data.label_key(ii) + ' %0.3f '%(np.abs((binnedY[ii]-binnedPred[ii]))/binnedY[ii]))\n","\n","  print(\"\\n\\n=========================================== Average Test Results ============================================\")\n","  dt = datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n","  print(f\"Test Time: {dt} | Average Test Loss: {(sum_results[0] / size_results)} | Average Test Accuracy: {(sum_results[1] / size_results)}\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UCOqTpiREif","colab_type":"code","outputId":"b84f5333-85ad-4327-b776-8aaedabc3cf0","executionInfo":{"status":"ok","timestamp":1589403675736,"user_tz":420,"elapsed":1771102,"user":{"displayName":"George Gabricht","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgF6OVYE4hVhnj7TIauxvLzLfbkf_0S-qHKAkoA=s64","userId":"04106723924461638279"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1LgC6bajRlx3zJ7ZaVVuAzMfR4wSiYDAE"}},"source":["data, histories, models, tests = cross_validate_model(augment=True)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"tNoKJICenSHA","colab_type":"code","colab":{}},"source":["if 'data' in locals() or 'data' in globals():\n","  analyze_results(data=data, histories=histories, models=models, tests=tests)\n","else:\n","  analyze_results(histories=histories, models=models, tests=tests)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oILkbwQLcMho","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","# ***Training a Final Model***\n","\n","\n","---\n","\n","\n","### **Instructions:**\n","1.   Run Cells 1 - 3\n","2.   Pick a Model in a define_model function and run that cell\n","3.   Run cells for augment_images and generate_final_model functions.\n","4.   Run the final cell which calls these functions, with desired params.\n","5.   This will train final model and save it to fer2013/models/TrainedModel.h5\n","6.   To Repeat for another model, repeat steps 2 - 5. WILL OVERWRITE MODEL!!!\n","\n","---"]},{"cell_type":"code","metadata":{"id":"U69qmYw6Tc0y","colab_type":"code","colab":{}},"source":["def generate_final_model(num_epochs=50, seed=10, batch_size=32, augment=False):\n","  data = fer2013()\n","  #data.pixels = np.reshape(data.pixels, (data.pixels.shape[0], 1, data.pixels.shape[1], data.pixels.shape[2]))\n","  #print(data.pixels.shape)\n","  #data.augment()\n","  data.shuffle(seed=int(0.8 * seed))\n","  data.shuffle(seed=seed)\n","  data.shuffle(seed=int(1.4 * seed))\n","  #print(data.pixels.shape)\n","  data.normalize()\n","\n","  #print(data.pixels.shape)\n","  #data.pixels = np.reshape(data.pixels, (data.pixels.shape[0], 1, data.pixels.shape[1], data.pixels.shape[2]))\n","    \n","  # %%\n","  startTime = time.time()\n","  train_norm, trainY = data.pixels, data.targets\n","  \n","  if (augment):\n","    train_norm, trainY = augment_images(train_norm, trainY, copyRatio=0.005, copyMult=1)\n","  else:\n","    train_norm, trainY = mirror_images(train_norm, trainY)\n","\n","  model = define_model(input_shape=train_norm[0].shape,output_shape=trainY.shape[1])\n","  plot_model(model, to_file='/content/drive/Shared drives/Emotion Classification Through Deep Learning Drive/fer2013/models/model_%f.png'%startTime, show_shapes=True, show_layer_names=True)\n","  print('Data Size/num. parameters: %0.3f'%(len(train_norm)/model.count_params()))\n","  # %%\n","  #history = model.fit(datagen.flow(train_norm, trainY, batch_size=BATCH_SIZE, shuffle=True), steps_per_epoch=len(train_norm) / 32, epochs = NUM_EPOCHS)\n","  model.fit(train_norm, trainY, epochs=num_epochs, batch_size=batch_size, verbose=1,shuffle=False)\n","  model.save('/content/drive/Shared drives/Emotion Classification Through Deep Learning Drive/fer2013/models/TrainedModel.h5')\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yQGem0xcKUa","colab_type":"code","outputId":"0fb033f8-4250-4aab-b72c-677c60a5101b","executionInfo":{"status":"ok","timestamp":1590625957476,"user_tz":420,"elapsed":850572,"user":{"displayName":"George Gabricht","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgF6OVYE4hVhnj7TIauxvLzLfbkf_0S-qHKAkoA=s64","userId":"04106723924461638279"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = generate_final_model()\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Data Size/num. parameters: 0.020\n","Epoch 1/50\n","71774/71774 [==============================] - 23s 318us/step - loss: 1.5972 - accuracy: 0.3721\n","Epoch 2/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 1.3627 - accuracy: 0.4803\n","Epoch 3/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 1.1767 - accuracy: 0.5558\n","Epoch 4/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 1.0023 - accuracy: 0.6302\n","Epoch 5/50\n","71774/71774 [==============================] - 16s 225us/step - loss: 0.8469 - accuracy: 0.6927\n","Epoch 6/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.7039 - accuracy: 0.7503\n","Epoch 7/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.5780 - accuracy: 0.7962\n","Epoch 8/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.4757 - accuracy: 0.8358\n","Epoch 9/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.3996 - accuracy: 0.8624\n","Epoch 10/50\n","71774/71774 [==============================] - 16s 228us/step - loss: 0.3364 - accuracy: 0.8858\n","Epoch 11/50\n","71774/71774 [==============================] - 16s 229us/step - loss: 0.2820 - accuracy: 0.9042\n","Epoch 12/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.2440 - accuracy: 0.9164\n","Epoch 13/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.2110 - accuracy: 0.9285\n","Epoch 14/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.1856 - accuracy: 0.9385\n","Epoch 15/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.1673 - accuracy: 0.9434\n","Epoch 16/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.1481 - accuracy: 0.9501\n","Epoch 17/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.1304 - accuracy: 0.9561\n","Epoch 18/50\n","71774/71774 [==============================] - 17s 233us/step - loss: 0.1213 - accuracy: 0.9595\n","Epoch 19/50\n","71774/71774 [==============================] - 16s 225us/step - loss: 0.1092 - accuracy: 0.9641\n","Epoch 20/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.1008 - accuracy: 0.9657\n","Epoch 21/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0940 - accuracy: 0.9681\n","Epoch 22/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0877 - accuracy: 0.9699\n","Epoch 23/50\n","71774/71774 [==============================] - 16s 225us/step - loss: 0.0842 - accuracy: 0.9719\n","Epoch 24/50\n","71774/71774 [==============================] - 16s 228us/step - loss: 0.0779 - accuracy: 0.9738\n","Epoch 25/50\n","71774/71774 [==============================] - 16s 228us/step - loss: 0.0679 - accuracy: 0.9775\n","Epoch 26/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0699 - accuracy: 0.9765\n","Epoch 27/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.0624 - accuracy: 0.9785\n","Epoch 28/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.0607 - accuracy: 0.9792\n","Epoch 29/50\n","71774/71774 [==============================] - 16s 229us/step - loss: 0.0580 - accuracy: 0.9804\n","Epoch 30/50\n","71774/71774 [==============================] - 16s 228us/step - loss: 0.0544 - accuracy: 0.9821\n","Epoch 31/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.0514 - accuracy: 0.9821\n","Epoch 32/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0511 - accuracy: 0.9825\n","Epoch 33/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0460 - accuracy: 0.9845\n","Epoch 34/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.0423 - accuracy: 0.9854\n","Epoch 35/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0462 - accuracy: 0.9843\n","Epoch 36/50\n","71774/71774 [==============================] - 16s 228us/step - loss: 0.0403 - accuracy: 0.9862\n","Epoch 37/50\n","71774/71774 [==============================] - 17s 232us/step - loss: 0.0401 - accuracy: 0.9862\n","Epoch 38/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0378 - accuracy: 0.9873\n","Epoch 39/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0359 - accuracy: 0.9879\n","Epoch 40/50\n","71774/71774 [==============================] - 16s 228us/step - loss: 0.0365 - accuracy: 0.9877\n","Epoch 41/50\n","71774/71774 [==============================] - 16s 225us/step - loss: 0.0348 - accuracy: 0.9885\n","Epoch 42/50\n","71774/71774 [==============================] - 16s 224us/step - loss: 0.0336 - accuracy: 0.9884\n","Epoch 43/50\n","71774/71774 [==============================] - 16s 226us/step - loss: 0.0321 - accuracy: 0.9888\n","Epoch 44/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0331 - accuracy: 0.9888\n","Epoch 45/50\n","71774/71774 [==============================] - 16s 229us/step - loss: 0.0323 - accuracy: 0.9893\n","Epoch 46/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0306 - accuracy: 0.9896\n","Epoch 47/50\n","71774/71774 [==============================] - 16s 229us/step - loss: 0.0302 - accuracy: 0.9898\n","Epoch 48/50\n","71774/71774 [==============================] - 16s 229us/step - loss: 0.0294 - accuracy: 0.9900\n","Epoch 49/50\n","71774/71774 [==============================] - 16s 227us/step - loss: 0.0277 - accuracy: 0.9904\n","Epoch 50/50\n","71774/71774 [==============================] - 16s 228us/step - loss: 0.0290 - accuracy: 0.9897\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 46, 46, 32)        320       \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 44, 44, 32)        9248      \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 44, 44, 32)        128       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 22, 22, 32)        0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 22, 22, 32)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 20, 20, 32)        9248      \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 18, 18, 32)        9248      \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 18, 18, 32)        128       \n","_________________________________________________________________\n","average_pooling2d_1 (Average (None, 9, 9, 32)          0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 9, 9, 32)          0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 2592)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1200)              3111600   \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 400)               480400    \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 100)               40100     \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 50)                5050      \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 7)                 357       \n","=================================================================\n","Total params: 3,665,827\n","Trainable params: 3,665,699\n","Non-trainable params: 128\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U34h6mmlpPty","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}